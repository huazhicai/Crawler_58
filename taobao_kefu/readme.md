### 目标
- 爬取58同城上，浙江省各市的淘宝/天猫/京东等电商客服的招聘企业信息
- 招聘信息(主要是电话号码)存入mysql数据库

### 环境
- Python 3.6.1
- pymysql
- scrapy 1.4
- scrapy-redis

### 设计思路
- 网页只有70页的索引资源，没法展示全部的资源，所以要按某个标准
分的更细，因为搜索的基本是互联网行业的数据，按行业划分没啥分流的效果，
所以采取按区分。 西湖区淘宝客服。。。
- 所提取的信息在多个页面，所以该怎么设计组织爬虫呢？
- 到详情页提取所需要信息，所以爬虫分两步：
    - url_spider(生产者): 提取所有城市详情页的url存储到主机redis中，以set形式保存，这样可以去重
    并可以断点续爬，因为被爬取的就删除了。
    - kefu_spider(消费者): 两个salver都从主机redis中获取url作为初始url ,爬取需要的数据资源
- 杭州的淘宝客服信息和其他所有城市差不多，每个城市弄个数据表没必要，所以就
分两个爬虫(分布式爬虫), 爬杭州数据的在一台电脑上，数据保存到主机数据库hz_kefu
数据表里， 其他城市的在另一台电脑爬取，数据保存到主机数据库qt_kefu数据表里


### 遇到的问题

- 最坑的是总是遇到访问过于频繁，请输入验证码
    - 就此问题尝试过很多方法： 使用ip代理池，减慢爬取的速度，减少进程数
    结果都不管用
    - 改变设计思路，就是分两个步骤进行，爬取闲情也url存入redis然后，再从redis
    取url爬，这样就减少访问操作频率了，但是还是不顶用。
    - 我一直觉得是不是ip限制的问题，因为Ip限制应该是目标服务器积极拒绝链接，
    这个问题明显是用户操作过于频繁。那就用户代理呗，我确实用了用户代理，但是
    他妹的还是解决不了这个问题。
    - 我很纳闷为啥前段时间爬取电话销售从未遇见这样的问题，想不出所以然，差点
    - 还好没放弃，一次调试异常的时候看到原来是**用户代理没成功**，终于找到原因了
    也符合我之前的猜想，`某个用户访问过于频繁` 解决的方式自然是用户代理
    - 但是为啥没生效呢，检查好几遍，没发现啥明显的问题。
    - 最后解决方式是**调低了用户代理系数**， 但是还是想不明白为什么

- 第二坑的问题是，parse函数里分流(self.parse_detail,self.parse_detail_mq)
    - 分流按照我的逻辑思路没有问题的呀，为啥爬虫就是不往下走了呢，
    在不太了解源代码的情况下，真觉得我的思路没问题，。
    - 只能在原码里打断点，一部一部找原因了，结果终于找出来了
    - **重写make_request_from_url覆盖源码函数**, 在写的这个函数里接受来自redis的url,从这里开始
    分开请求
    - 解析函数返回的是yield， 断点调试很重要。

- 第三坑的是: 用Rule(linkExtractor())提取首页浙江各城市的链接
    - 死都提取不出来，打印response才发现，返回的是一堆javascripts
    毛都提取不出来
    - 然后最简单的手动复制黏贴

- 第四坑的是：用正则在详情匹配客户和薪资
    - 复制返回的response.text 在正则中各种尝试，结果都不能失望了，
    主要是这则写的不对，记得中文匹配[] 。结果虽然勉强可以，但是效果不是很好
    - 然后发现，可以选择器配合这则操作，这太方便了
    - response.xpath().re()/re_first()
    - 更方便的是response.xpath('//a[contains(@title, "客服")]/text()').extract_first(default='淘宝客服')
    - contains实在太好用了"//a[contains(., 'Next Page')]"

- 第五坑的是，数据保存不到mysql
    - 前面运行都正常，也没啥其他异常，但是程序就是不走到pipelines
    数据存储的地方。
    - 有点纳闷，检查也没发觉啥异常
    - 最后做了两点修正，item里的scrapy.item.Item 换成scrapy.Item,
    前者好像是在CrawlerSpider的爬虫里用到的，
    - 最可能的原因还是，配置系数的修改，配置系数有先后顺序

- 第六坑: 自己有点坑，
    - 有时候陷入到死胡同，一根筋。一直陷入，耗费大量时间，而且做出结果的意义
    不大，比如图像电话号码识别。怎么尝试都没啥改进，最后还是更新识别引擎
    稍微有点好转。而且电话图片资源本来就不多花费大量时间有点不值得了。

### 用到的知识点
    - Rule()
    - 重写 make_request_from_url
    - '//a[containes(@title, "客服")]/../../following-sibling::td[1]'
    - 用户代理
    - 爬虫动荡延时3*0.5~3*1.5
    - redis去重
    - 想用ItemLoader技能

### 模糊知识点
- redis如何去重的
- redis如何分布式爬虫
- ItemLoader如何使用
- 如何分别写入不同的数据库
